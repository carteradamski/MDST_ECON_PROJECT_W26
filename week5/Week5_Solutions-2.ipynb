{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0823602b",
   "metadata": {},
   "source": [
    "# Week 5 — Universal Modeling (Regression) (40–60 minutes)\n",
    " Works for **monthly or quarterly** datasets\n",
    "\n",
    "## Today’s goals\n",
    "1) Choose target (level vs change)\n",
    "2) Choose small feature set (3–8 cols)\n",
    "3) Train baseline regression\n",
    "4) Evaluate with time split\n",
    "5) Interpret coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eec6ce",
   "metadata": {},
   "source": [
    "## Quick Python Basics Recap (for this week)\n",
    "\n",
    "This notebook assumes **you are still new to Python**. Below is the minimum syntax you need today.\n",
    "\n",
    "### DataFrames (pandas)\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"your_file.csv\")\n",
    "df.head()            # shows first rows\n",
    "df.columns           # column names\n",
    "df[\"col_name\"]       # select one column (a Series)\n",
    "df[[\"a\",\"b\"]]        # select multiple columns (a DataFrame)\n",
    "df.isna().sum()      # missing values per column\n",
    "```\n",
    "\n",
    "### Making plots (matplotlib)\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"x\"], df[\"y\"])   # line plot\n",
    "plt.title(\"Title\")\n",
    "plt.xlabel(\"x label\")\n",
    "plt.ylabel(\"y label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/example.png\", dpi=150)\n",
    "plt.show()\n",
    "```\n",
    "Key idea: **You build a plot step-by-step**, then save it with `savefig`.\n",
    "\n",
    "### Writing comments\n",
    "- Use `#` for a comment on one line.\n",
    "- In this course, you must explain what your code does and what you learned from each plot.\n",
    "\n",
    "### Strings and f-strings (for readable printing)\n",
    "```python\n",
    "value = 3.14\n",
    "print(f\"The value is {value}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d150021",
   "metadata": {},
   "source": [
    "## Final Project Artifacts (you must create these)\n",
    "\n",
    "By the end of this notebook, you must have:\n",
    "1. At least **2 saved figures** in the `figures/` folder (PNG files).\n",
    "2. A short **Insights** write-up answering: What changed? What matters? What would you model next?\n",
    "\n",
    "If you cannot find `figures/`, create it using:\n",
    "```python\n",
    "import os\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff444bf",
   "metadata": {},
   "source": [
    "## Syntax Toolbox for Week 5 (Modeling)\n",
    "\n",
    "This week introduces machine learning code. Here is the syntax before you use it.\n",
    "\n",
    "### 1) Selecting feature columns (X) and target (y)\n",
    "```python\n",
    "X = df[[\"feat1\", \"feat2\"]]\n",
    "y = df[\"target\"]\n",
    "```\n",
    "\n",
    "### 2) Train/test split\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "```\n",
    "- The model learns from `train` and is evaluated on `test`.\n",
    "\n",
    "### 3) Fit a model and predict\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### 4) Evaluate with MAE / RMSE\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "```\n",
    "\n",
    "### 5) A scatter plot to check prediction quality\n",
    "```python\n",
    "plt.scatter(y_test, pred)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1386c02",
   "metadata": {},
   "source": [
    "> **Expanded version** (generated 2026-01-05). Added extra coding + commenting + writing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82e75f",
   "metadata": {},
   "source": [
    "## 1) Setup + load + engineer (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41985104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed (first time only):\n",
    "# !pip -q install pandas_datareader scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 4)  # Create a plot for interpretation / reporting\n",
    "plt.rcParams[\"axes.grid\"] = True  # Create a plot for interpretation / reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2edcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "from sklearn.linear_model import LinearRegression  # Fit a model (baseline or predictive)\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "def fetch_fred_series(series_id: str, start=\"1990-01-01\", end=None) -> pd.DataFrame:\n",
    "    \"\"\"Fetch one FRED series as a DataFrame with a datetime index.\"\"\"\n",
    "    if end is None:\n",
    "        end = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
    "    s = pdr.DataReader(series_id, \"fred\", start, end)\n",
    "    s.columns = [series_id]\n",
    "    s.index = pd.to_datetime(s.index)  # Ensure date/time column is parsed correctly\n",
    "    return s\n",
    "\n",
    "def fetch_many(series_ids, start=\"1990-01-01\"):\n",
    "    dfs = [fetch_fred_series(s, start=start) for s in series_ids]\n",
    "    return pd.concat(dfs, axis=1).sort_index()\n",
    "\n",
    "def infer_freq(index: pd.DatetimeIndex) -> str:\n",
    "    f = pd.infer_freq(index)\n",
    "    if f is None:\n",
    "        return \"U\"\n",
    "    f = f.upper()\n",
    "    if \"Q\" in f:\n",
    "        return \"Q\"\n",
    "    if \"M\" in f:\n",
    "        return \"M\"\n",
    "    return \"U\"\n",
    "\n",
    "def to_period_end(df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    # Default: last observation within each period.\n",
    "    if target == \"M\":\n",
    "        return df.resample(\"M\").last()\n",
    "    if target == \"Q\":\n",
    "        return df.resample(\"Q\").last()\n",
    "    raise ValueError(\"target must be 'M' or 'Q'\")\n",
    "\n",
    "def add_common_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        out[f\"{c}_lag1\"] = out[c].shift(1)\n",
    "        out[f\"{c}_diff1\"] = out[c].diff(1)\n",
    "        out[f\"{c}_pct1\"] = out[c].pct_change(1) * 100\n",
    "        out[f\"{c}_roll3\"] = out[c].rolling(3).mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec88ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# STUDENT CHOICE (EDIT HERE)\n",
    "# ===========================\n",
    "# Choose 3–6 FRED series IDs relevant to your question.\n",
    "# Search on https://fred.stlouisfed.org and copy the series ID.\n",
    "\n",
    "series_ids = [\n",
    "    \"UNRATE\",\n",
    "    \"CPIAUCSL\",\n",
    "    \"FEDFUNDS\"\n",
    "]\n",
    "\n",
    "# Choose your target variable (must be one of the series_ids)\n",
    "target_id = \"CPIAUCSL\"\n",
    "\n",
    "start_date = \"1990-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df99be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = fetch_many(series_ids, start=start_date)\n",
    "\n",
    "# Infer each series' frequency\n",
    "freqs = {c: infer_freq(df_raw[c].dropna().index) for c in df_raw.columns}  # Handle missing values\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule: if any series is quarterly, use quarterly for everything (safe when mixing).\n",
    "use_freq = \"Q\" if any(v == \"Q\" for v in freqs.values()) else \"M\"\n",
    "print(\"Using frequency:\", use_freq)\n",
    "\n",
    "df = to_period_end(df_raw, use_freq)\n",
    "\n",
    "# Drop rows where target is missing (we can’t model without target)\n",
    "df = df.dropna(subset=[target_id])  # Handle missing values\n",
    "\n",
    "# Missing-value strategies:\n",
    "df_complete = df.dropna()                 # simplest: keep only complete rows\n",
    "df_ffill = df.fillna(method=\"ffill\")      # common: forward-fill predictors\n",
    "\n",
    "#  Choose ONE:\n",
    "df_use = df_complete   # or df_ffill\n",
    "\n",
    "df_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "df_feat = add_common_features(df_use[series_ids]).dropna()  # Handle missing values\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a267e0",
   "metadata": {},
   "source": [
    "## 2) Choose y (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choose level/pct/diff\n",
    "y = df_feat[f\"{target_id}_pct1\"]\n",
    "y.name = \"y\"\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d26e81",
   "metadata": {},
   "source": [
    "## 3) Choose X (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8282ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "predictors = [s for s in series_ids if s != target_id]\n",
    "cols=[]\n",
    "for s in predictors:\n",
    "    cols += [f\"{s}_lag1\", f\"{s}_pct1\", f\"{s}_roll3\"]\n",
    "\n",
    "X = df_feat[cols].copy()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b70ad6",
   "metadata": {},
   "source": [
    "## 4) Time split (8–10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "cut = int(len(df_feat)*0.8)\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451cfbc",
   "metadata": {},
   "source": [
    "## 5) Fit + evaluate (10–12 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1adc70",
   "metadata": {},
   "source": [
    "###  Baseline Model (Required)\n",
    "Before using regression, build a **baseline** so you can say whether your model is actually helpful.\n",
    "\n",
    "Pick one baseline:\n",
    "- predict the training mean of y\n",
    "- \"last value\" baseline (ŷ_t = y_{t-1})\n",
    "- simple moving average baseline\n",
    "\n",
    "**Deliverable:** compute baseline MAE and compare to your regression MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ef012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (STUDENTS):\n",
    "# Implement one baseline and compute MAE on the test set.\n",
    "# Print a comparison:\n",
    "# - baseline MAE\n",
    "# - regression MAE\n",
    "# - % improvement (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "# SOLUTION (INSTRUCTOR): Example implementation scaffold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = df_use.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", num_cols[:10])\n",
    "\n",
    "# Choose columns safely\n",
    "col = num_cols[0]\n",
    "fig, ax = plt.subplots(figsize=(10,4))  # Create a plot for interpretation / reporting\n",
    "df_use[col].plot(ax=ax)  # Create a plot for interpretation / reporting\n",
    "ax.set_title(f\"Example plot for {col}\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "fig.tight_layout()\n",
    "plt.show()  # Create a plot for interpretation / reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566fa12",
   "metadata": {},
   "source": [
    "###  Pipeline Practice (Required)\n",
    "Build a scikit-learn **Pipeline** that:\n",
    "1) imputes missing values (if any)\n",
    "2) scales features\n",
    "3) fits a model\n",
    "\n",
    "Then try **two models**:\n",
    "- LinearRegression (baseline)\n",
    "- Ridge OR Lasso (regularized)\n",
    "\n",
    "**Deliverable:** which one generalizes better on the test set? Why might that be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6862169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (STUDENTS):\n",
    "# Create a Pipeline using sklearn.\n",
    "# Fit LinearRegression and Ridge (or Lasso) and compare test MAE/RMSE.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso  # Fit a model (baseline or predictive)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# pipe_lr = Pipeline([...])\n",
    "# pipe_ridge = Pipeline([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67854f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "# SOLUTION (INSTRUCTOR): Example implementation scaffold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = df_use.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", num_cols[:10])\n",
    "\n",
    "# Choose columns safely\n",
    "col = num_cols[0]\n",
    "fig, ax = plt.subplots(figsize=(10,4))  # Create a plot for interpretation / reporting\n",
    "df_use[col].plot(ax=ax)  # Create a plot for interpretation / reporting\n",
    "ax.set_title(f\"Example plot for {col}\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "fig.tight_layout()\n",
    "plt.show()  # Create a plot for interpretation / reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3419cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)  # Fit a model (baseline or predictive)\n",
    "pred_test = model.predict(X_test)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, pred_test))\n",
    "print(\"Test R2 :\", r2_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5bf62",
   "metadata": {},
   "source": [
    "## 6) Coefficients (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca8676",
   "metadata": {},
   "source": [
    "###  Interpretation Drill (Required)\n",
    "Choose **two** coefficients and explain, in plain English:\n",
    "- what a 1-unit increase in X means (units!)\n",
    "- how y changes (direction + magnitude)\n",
    "- whether that seems realistic\n",
    "\n",
    "Also add a note: could multicollinearity be affecting coefficients?\n",
    "\n",
    "\n",
    "Write at least 3 sentences. Include: (1) what you observe, (2) why it might be happening, (3) how it affects your modeling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e901321",
   "metadata": {},
   "source": [
    "###  Optional Stretch: Time Series Cross-Validation\n",
    "Use `TimeSeriesSplit` to evaluate your model across multiple splits.\n",
    "\n",
    "**Deliverable:** a table of MAE per split + average MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e658a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (STUDENTS, optional stretch):\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "# maes = []\n",
    "# for train_idx, test_idx in tscv.split(X):\n",
    "#     ...\n",
    "# print(maes, np.mean(maes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "coef_tbl = pd.DataFrame({\"feature\": X.columns, \"coef\": model.coef_})\n",
    "coef_tbl[\"abs_coef\"] = coef_tbl[\"coef\"].abs()\n",
    "coef_tbl = coef_tbl.sort_values(\"abs_coef\", ascending=False)\n",
    "coef_tbl.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1490fc3",
   "metadata": {},
   "source": [
    "**Instructor note:** stress association language and units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e1e29",
   "metadata": {},
   "source": [
    "## 7) Residual plot (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b3dda",
   "metadata": {},
   "source": [
    "###  Residual Diagnostics (Required)\n",
    "Compute and comment on:\n",
    "- residual mean (should be near 0)\n",
    "- residual std\n",
    "- whether residuals get larger when predictions are larger (heteroskedasticity)\n",
    "\n",
    "**Deliverable:** 3–5 sentences explaining whether your model assumptions look reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (STUDENTS):\n",
    "# Compute residual summary stats and print them clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "# SOLUTION (INSTRUCTOR): Example implementation scaffold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = df_use.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", num_cols[:10])\n",
    "\n",
    "# Choose columns safely\n",
    "col = num_cols[0]\n",
    "fig, ax = plt.subplots(figsize=(10,4))  # Create a plot for interpretation / reporting\n",
    "df_use[col].plot(ax=ax)  # Create a plot for interpretation / reporting\n",
    "ax.set_title(f\"Example plot for {col}\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "fig.tight_layout()\n",
    "plt.show()  # Create a plot for interpretation / reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "resid = y_test - pred_test\n",
    "plt.figure()  # Create a plot for interpretation / reporting\n",
    "plt.plot(resid.index, resid.values)  # Create a plot for interpretation / reporting\n",
    "plt.title(\"Residuals over time (test)\")  # Create a plot for interpretation / reporting\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Residual (y - ŷ)\")  # Create a plot for interpretation / reporting\n",
    "plt.show()  # Create a plot for interpretation / reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be090c81",
   "metadata": {},
   "source": [
    "## 8) Reflection (8–10 min)\n",
    "1) Performance?\n",
    "2) Important feature?\n",
    "3) Next step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656ef8e",
   "metadata": {},
   "source": [
    "**Instructor example:** Try more lags, different y, add domain predictors; keep explainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae69e2",
   "metadata": {},
   "source": [
    "## End-of-class checkpoint\n",
    " baseline model + interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4cfff",
   "metadata": {},
   "source": [
    "## Final Project Artifacts (Save these for your report)\n",
    "\n",
    "By the end of the project, you should have:\n",
    "- At least 2 polished figures that show *trends* and *relationships*\n",
    "- A small table of your **top correlations** with the target\n",
    "- A short written interpretation of what the plots suggest\n",
    "\n",
    "In this section you will generate and save figures you can reuse in your final write-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Goal: Run and understand the steps below\n",
    "# - What you should check after running:\n",
    "#   1) The output has the expected shape/columns\n",
    "#   2) The values look reasonable (no obvious NaNs or impossible values)\n",
    "#   3) Any figures have clear titles/labels and are saved to disk when required\n",
    "#\n",
    "# How to read this code:\n",
    "# - Imports / configuration come first\n",
    "# - Then we compute intermediate variables (feature engineering)\n",
    "# - Then we summarize / visualize\n",
    "# - Finally, we write a short interpretation in Markdown below the figure/table\n",
    "\n",
    "# SOLUTION (INSTRUCTOR):\n",
    "# Save a trend figure and a relationship figure using available numeric columns.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)  # Create output folder if it does not exist\n",
    "\n",
    "num_cols = df_use.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(num_cols) == 0:\n",
    "    raise ValueError(\"No numeric columns found in df_use. Check data loading/cleaning steps.\")\n",
    "\n",
    "# Trend figure: first numeric column\n",
    "col_trend = num_cols[0]\n",
    "fig, ax = plt.subplots(figsize=(10,4))  # Create a plot for interpretation / reporting\n",
    "df_use[col_trend].plot(ax=ax)  # Create a plot for interpretation / reporting\n",
    "ax.set_title(f\"Trend of {col_trend} over time\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/trend_{col_trend}.png\", dpi=200)  # Save an artifact you can reuse in the final project\n",
    "\n",
    "# Relationship figure: correlation heatmap (top 8 numeric cols)\n",
    "use_cols = num_cols[:8]\n",
    "corr = df_use[use_cols].corr()  # Compute correlations to look for relationships\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,6))  # Create a plot for interpretation / reporting\n",
    "im = ax.imshow(corr.values)\n",
    "ax.set_xticks(range(len(use_cols)))\n",
    "ax.set_yticks(range(len(use_cols)))\n",
    "ax.set_xticklabels(use_cols, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(use_cols)\n",
    "ax.set_title(\"Correlation heatmap (subset of variables)\")\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/corr_heatmap.png\", dpi=200)  # Save an artifact you can reuse in the final project\n",
    "\n",
    "corr.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2911f",
   "metadata": {},
   "source": [
    "### Written Insights (Required)\n",
    "\n",
    "Write 5–8 bullet points answering:\n",
    "1. Which variable trends most strongly over time? What might explain it?\n",
    "2. Which pair of variables looks most related? Is that relationship stable over time?\n",
    "3. What missingness or outliers could bias modeling?\n",
    "4. What is one feature engineering idea you want to try next week?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ec34e",
   "metadata": {},
   "source": [
    "#### Sample instructor bullets (example)\n",
    "\n",
    "- The first numeric series shows a clear upward trend; this suggests non-stationarity and motivates using percent change or differencing.\n",
    "- Correlation heatmap indicates several variables move together, suggesting multicollinearity; regularization may help.\n",
    "- Missingness is concentrated in a small set of columns; imputation strategy should be justified and tested.\n",
    "- Outliers appear around major economic events; consider robust scaling or winsorization and document the rationale.\n",
    "- Next week: create lag features and rolling aggregates to capture delayed effects.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week5_Universal_Paced_Instructor",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
